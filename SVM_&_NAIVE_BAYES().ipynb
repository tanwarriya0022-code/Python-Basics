{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPs4UEKgX0o10r817uad+Hj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanwarriya0022-code/Python-Basics/blob/main/SVM_%26_NAIVE_BAYES().ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNCE9tyRYn-0"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Information Gain, andWhat is Information Gain, and how is it used in Decision Trees?\n",
        " how is it used in Decision Trees?\n",
        " - 1. The Foundation: EntropyTo understand Information Gain, you first need to understand Entropy.5 In machine learning, Entropy is a measure of \"impurity\" or \"disorder.\"6+1Low Entropy: If a group contains only one type of data (e.g., all \"Yes\" votes), the entropy is 0 (perfectly pure).7High Entropy: If a group is a 50/50 mix of \"Yes\" and \"No\" votes, the entropy is 1 (totally messy/uncertain).8The formula for Entropy ($H$) for a dataset ($S$) with $c$ classes is:$$H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$$Where 9$p_i$ is the probability (proportion) of class 10$i$ in the dataset.112. Calculating Information GainInformation Gain is simply the difference between the entropy of the parent node and the weighted average entropy of the child nodes after a split.The formula is:$$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$$$H(S)$: Entropy of the parent node before the split.12$\\sum \\frac{|S_v|}{|S|} H(S_v)$: The weighted sum of the entropy of each child node created by the split.13The Rule: The attribute with the highest Information Gain is selected for the split because it reduces the most uncertainty.143. How It’s Used in Decision TreesAlgorithms like ID3 use Information Gain to build the tree through a process called Recursive Partitioning:15Calculate Entropy for the current set of data.Calculate Information Gain for every available feature (e.g., \"Is it Sunny?\", \"Is it Windy?\").16Pick the winner: Choose the feature with the highest Information Gain as the decision node.17Split the data into branches based on that feature.18Repeat the process for each branch until the data is fully classified or a stopping condition is met.19Comparison: Information Gain vs. Gini ImpurityWhile Information Gain is common, many modern libraries (like Scikit-Learn) use Gini Impurity by default.FeatureInformation Gain (Entropy)Gini ImpurityMathUses logarithms ($log_2$).Uses squares ($p^2$).SpeedSlightly slower (log calculations).Faster to calculate.BiasFavors features with many categories.Favors larger partitions.Tree ResultOften results in more balanced trees.Tends to isolate the most frequent class."
      ],
      "metadata": {
        "id": "4DARXIA2ZbX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Gini Impurity and Entropy?\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases.\n",
        "- Mathematical ComparisonThe primary difference is that Gini uses squared probabilities (linear), while Entropy uses logarithms (curved).2FeatureGini ImpurityEntropyFormula$G = 1 - \\sum p_i^2$$H = -\\sum p_i \\log_2(p_i)$CalculationUses simple subtraction and squares.Uses logarithmic functions.Range0.0 to 0.5 (for binary classes).0.0 to 1.0 (for binary classes).ComplexityLow (faster).High (slower due to $\\log$).2. Key Differences in BehaviorWhile the results are often identical, they have subtle \"personalities\" that emerge in edge cases:Sensitivity: Entropy is more sensitive to changes in class probabilities.3 Because the $\\log$ function is steeper than a squared function, Entropy \"penalizes\" impurity more heavily. This can lead to more balanced trees.Class Isolation: Gini tends to isolate the most frequent class into its own branch.4 It focuses on reducing misclassification probability.Computation: Gini is the default in many libraries (like Scikit-Learn) simply because it is faster to calculate.5 For massive datasets with hundreds of features, the time saved by avoiding $\\log$ calculations adds up significantly.3. Summary: Strengths and WeaknessesGini ImpurityStrengths: Extremely fast; easy to implement; works well for large datasets.6Weaknesses: Can be biased toward features with many categories; might not capture subtle distribution differences as well as Entropy.7Use Case: Large-scale data, real-time applications, or when using standard Random Forest models.EntropyStrengths: Provides a more \"fine-grained\" measure of information; better for exploratory analysis where understanding the \"surprise\" or \"uncertainty\" in the data is key.8Weaknesses: Computationally expensive; can take longer to train on very deep trees.9Use Case: Smaller datasets where model precision is more important than training speed, or when using the C4.5 algorithm.\n",
        "\n"
      ],
      "metadata": {
        "id": "ErVQzNE6ZeID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Pre-Pruning in Decision Trees?\n",
        "- Pre-pruning, also known as Early Stopping, is a technique used to prevent a Decision Tree from becoming too complex and overfitting the training data.1Unlike standard tree building—which grows until every leaf is \"pure\" or out of data—pre-pruning halts the growth of the tree during the construction phase as soon as certain pre-defined conditions are met.21. How Pre-Pruning WorksDuring the recursive process of splitting nodes, the algorithm evaluates \"stopping criteria\" at every step.3 If a potential split doesn't meet these rules, the node is turned into a leaf node immediately, and no further branches are created from that point.2. Common Stopping Criteria (Hyperparameters)4In libraries like Scikit-Learn, you implement pre-pruning by tuning these specific hyperparameters:5Maximum Depth (max_depth): Limits how many levels deep the tree can grow.6Minimum Samples for Split (min_samples_split):7 A node will only split if it contains at least a certain number of data points.8+1Minimum Samples per Leaf (min_samples_leaf):9 Ensures that every final \"leaf\" has a minimum number of samples, preventing the tree from isolating single outliers.10Maximum Leaf Nodes (max_leaf_nodes):11 Limits the total number of leaves in the entire tree.Minimum Impurity Decrease: A split is only allowed if it reduces the Gini Impurity or Entropy by a significant amount (e.g., more than 0.001).3. Pros and ConsFeaturePre-Pruning (Early Stopping)SpeedFaster: You stop building the tree early, saving computation time.ComplexitySimpler: Results in smaller, more interpretable trees.RiskUnderfitting: If you stop too early (e.g., max_depth=2), the tree might miss important patterns in the data.EfficiencyMemory efficient: Since the full tree is never built, it requires less RAM.4. Pre-Pruning vs. Post-PruningWhile Pre-Pruning stops the tree while it is still growing, Post-Pruning (or \"Backward Pruning\") allows the tree to grow to its full, messy extent first.12 Only after it is finished does the algorithm go back and \"trim\" branches that don't add significant predictive power."
      ],
      "metadata": {
        "id": "ZyciunaCZgu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "(Include your Python code and output in the code box below.)\n",
        "-"
      ],
      "metadata": {
        "id": "ow-8Nq4LZnRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is a Support Vector Machine (SVM)?\n",
        "- A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm primarily used for classification, though it can also handle regression tasks.1Think of an SVM as a \"boundary seeker.\" Its goal is to find the best possible line (or space) that separates different classes of data with the widest possible gap.21. Key Components of an SVMTo understand how it works, you need to know three main terms:Hyperplane: This is the decision boundary that separates the classes.3 In 2D, it's a line; in 3D, it's a flat plane; and in higher dimensions, it's called a hyperplane.4+1Support Vectors: These are the data points located closest to the hyperplane.5 They are \"critical\" because if you move them, the boundary moves.6 The rest of the data points don't actually affect the position of the boundary.+1Margin: This is the distance between the hyperplane and the nearest support vectors.7 SVM tries to maximize this margin to ensure the model generalizes well to new data.8+12. The \"Kernel Trick\" (Handling Non-Linear Data)9In the real world, data is rarely perfectly separable by a straight line.10 Imagine blue dots in a circle surrounded by a ring of red dots. You can't draw a single line to separate them in 2D.The Kernel Trick allows SVM to map the data into a higher dimension (e.g., from 2D to 3D).11 Once the data is lifted into this new space, a flat hyperplane can suddenly separate the classes easily.12+1Common Kernels include:Linear: Used when data is already linearly separable.13Polynomial: Good for curved boundaries.14RBF (Radial Basis Function): The most popular choice; it can handle complex, overlapping data by mapping it into an infinite-dimensional space.3. Pros and Cons of SVMAdvantagesDisadvantagesHigh Accuracy: Very effective in high-dimensional spaces.Slow Training: Can be very slow on large datasets (thousands of rows+).Memory Efficient: Only uses support vectors (a subset of points) to define the boundary.Hard to Interpret: The final model is a \"black box\" compared to a Decision Tree.Robust to Outliers: Since it focuses on the margin, it isn't easily swayed by far-away noise.15Sensitive to Noise: If classes overlap heavily, the \"soft margin\" becomes hard to tune.4. When to use SVM?SVM is a top choice for complex but small-to-medium datasets, such as:Text Classification: Spam detection or sentiment analysis.16Image Recognition: Face detection or handwriting analysis.Bioinformatics: Protein fold and remote homology detection."
      ],
      "metadata": {
        "id": "SwrDrtSwaEWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the Kernel Trick in SVM?\n",
        "- The Kernel Trick is the \"magic\" that allows a Support Vector Machine (SVM) to solve complex, non-linear problems using simple linear math.1Normally, an SVM is a linear classifier, meaning it can only separate data by drawing a straight line (or a flat plane). However, real-world data is often \"tangled\" in a way that no straight line can separate it.21. The Core Concept: Lifting the Data3The intuition behind the kernel trick is that data that is inseparable in a lower dimension (like 2D) might become easily separable if you \"lift\" it into a higher dimension (like 3D).42D Problem: Imagine red dots in the center of a circle and blue dots forming a ring around them (the \"donut\" problem).5 No straight line can separate them.6+13D Solution: Imagine lifting the red dots up along a new 7$z$-axis (representing distance from the center).8 Now, you can slide a flat sheet of paper (a hyperplane) between the red dots \"floating\" in the air and the blue dots staying on the ground.9+12. Why is it a \"Trick\"?The \"trick\" is a mathematical shortcut. Transforming thousands of data points into a 100-dimensional or even infinite-dimensional space is computationally \"expensive\" (it would crash your computer or take forever).10The Magic: The SVM algorithm only needs the dot product (a measure of similarity) between pairs of points to find the boundary.11 A Kernel Function allows us to calculate what that dot product would be in a high-dimensional space without actually moving the data there.12+1Simplified: It’s like being able to tell how close two people are standing in a 3D room just by looking at their 2D shadows on the floor. You get the 3D information without ever leaving the 2D floor.3. Common Kernel FunctionsDifferent kernels are used depending on the shape of your data:KernelUse CaseResulting BoundaryLinearWhen data is already separated by a line.Straight LinePolynomialUseful for curved or circular boundaries.Curved/Complex shapesRBF (Gaussian)The most popular; handles almost any shape.Bubbles or \"Islands\"Sigmoid13Often used as a proxy for Neural Networks.14S-shaped curves4. Summary of BenefitsEfficiency: You get the power of high-dimensional math with the speed of low-dimensional computation.15Flexibility: It allows SVMs to adapt to incredibly complex data patterns that other linear models can't touch.16Infinite Dimensions: Some kernels (like RBF) technically map data to an infinite-dimensional space, which would be physically impossible to compute without the tric"
      ],
      "metadata": {
        "id": "ABz3piKEaT3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset.\n",
        "(Include your Python code and output in the code box below.)\n",
        "-"
      ],
      "metadata": {
        "id": "-gFfC6SsaWoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "- The Naïve Bayes classifier is a popular supervised machine learning algorithm used for classification tasks, such as spam detection, sentiment analysis, and document categorization.1 It is based on the Bayes’ Theorem, a fundamental principle in probability.21. The Core Idea: Bayes' TheoremAt its heart, Naïve Bayes calculates the probability of a \"class\" (like \"Spam\") given a set of \"features\" (like the words in an email). It answers the question: Given these observed features, which category is most likely?The mathematical formula for Bayes' Theorem is:$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$In the context of machine learning, we often write it as:$P(c|x)$: The probability of class $c$ given predictor $x$ (Posterior Probability).$P(x|c)$: The probability of predictor 3$x$ given class 4$c$ (Likelihood).5$P(c)$: The prior probability of class 6$c$ (Prior).7$P(x)$: The prior probability of predictor 8$x$ (Evidence).92. Why is it called \"Naïve\"?The algorithm is called \"naïve\" because it makes a massive, often unrealistic assumption: Feature Independence.10It assumes that the presence of a particular feature in a class is completely unrelated to the presence of any other feature.11Example: Identifying an AppleA fruit may be considered an apple if it is red, round, and about 3 inches in diameter.In reality, these features are often related (e.g., certain varieties are always both red and small).A Naïve Bayes classifier, however, treats \"Red,\" \"Round,\" and \"Size\" as independent contributors to the probability that the fruit is an apple, regardless of any possible correlations between them.Despite this \"naïve\" assumption, the algorithm performs surprisingly well on complex real-world problems, especially in text classification.123. Types of Naïve Bayes ModelsDepending on the distribution of your data, you choose a different flavor of the algorithm:ModelData TypeCommon Use CaseGaussianContinuous values (follows Normal distribution).Predicting iris flower types or physical measurements.MultinomialDiscrete counts (word frequencies).Text classification and spam filtering.BernoulliBinary/Boolean features (Yes/No).Document classification where only word presence matters.4. Pros and ConsPros:Blazing Fast: It is much faster than more complex algorithms like SVM or Random Forests.Low Data Requirements: It performs well even with a small amount of training data.13Scalability: It handles high-dimensional data (thousands of features) very effectively.14Cons:The \"Zero Frequency\" Problem: If a category and a feature never occur together in your training data, the probability drops to zero, and the model can't make a prediction. (This is usually fixed with \"Laplace Smoothing\").Bad Probability Estimations: While it’s great at picking the right class, the actual probability percentages it outputs are often not very accurate."
      ],
      "metadata": {
        "id": "JgdQhzuKanPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "- The Naïve Bayes classifier is a collection of classification algorithms based on Bayes’ Theorem.1 It is a generative model that is widely used for text classification, spam filtering, and sentiment analysis because it is incredibly fast and efficient.2+11. The Core Idea: Bayes' TheoremAt its heart, Naïve Bayes calculates the probability of a \"class\" (e.g., Spam)3 given a set of \"features\" (e.g., words like Winner, Free, Cash). It answers the question: Given these observed features, which category is most likely?The mathematical formula for Bayes' Theorem is:$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$In machine learning terms, we calculate the Posterior Probability:$$P(\\text{class}|\\text{features}) = \\frac{P(\\text{features}|\\text{class}) \\cdot P(\\text{class})}{P(\\text{features})}$$2. Why is it called \"Naïve\"?The algorithm is called \"naïve\" because it makes a massive, often unrealistic assumption: Strong Feature Independence.4It assumes that the presence of a particular feature in a class is completely unrelated to the presence of any other feature.5The Apple Example:A fruit may be considered an apple if it is red, round, and 3 inches in diameter.In reality, these features are often correlated (e.g., certain varieties are always red when they reach a specific size).A Naïve Bayes classifier treats \"Red,\" \"Round,\" and \"Size\" as independent contributors to the probability that the fruit is an apple, regardless of any relationships between them.Despite this \"naïve\" assumption, the algorithm performs surprisingly well on complex real-world problems because the simplified math still manages to capture the general trend of the data.63. Types of Naïve Bayes ModelsDepending on the nature of your features, you use different versions of the algorithm:ModelData TypeCommon Use CaseGaussianContinuous values (follows a bell curve).Predicting physical measurements (height/weight).MultinomialDiscrete counts (word frequencies).Text Classification (Spam vs. Not Spam).BernoulliBinary/Boolean (Yes/No).Document classification where only word presence matters.4. Pros and ConsPros:Speed: It is one of the fastest supervised learning algorithms.7Scale: It handles high-dimensional data (thousands of features) effortlessly.8Low Data Requirements: It can perform well even with a relatively small training set.9Cons:The \"Zero Frequency\" Problem: If a category and a feature never occur together in your training data, the probability drops to zero, and the model fails. (This is usually fixed with Laplace Smoothing).Poor Probability Estimates: It’s great at picking the right class, but the actual probability numbers it outputs are often not reliable.10"
      ],
      "metadata": {
        "id": "D2NgFeOFa4DE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "(Include your Python code and output in the code box below.)\n",
        "-"
      ],
      "metadata": {
        "id": "iiCZXe9GbHMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rBytkFiTa3US"
      }
    }
  ]
}