{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWA/mNW4l47W9702sYdX1Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanwarriya0022-code/Python-Basics/blob/main/EDA0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the difference between AI, ML, DL, and Data Science? Provide a\n",
        "brief explanation of each.\n",
        "(Hint: Compare their scope, techniques, and applications for each.)\n",
        "\n",
        "**1. Artificial Intelligence (AI)**\n",
        "\n",
        " **Scope**\n",
        "- The broadest field.\n",
        "- Goal: Build machines that can perform tasks requiring human intelligence.\n",
        "\n",
        "**Techniques**\n",
        "- Rule-based systems\n",
        "- Search algorithms\n",
        "- Optimization\n",
        "- Machine learning and deep learning (subfields)\n",
        "\n",
        "**Applications**\n",
        "- Robotics\n",
        "- Expert systems\n",
        "- Chatbots\n",
        "- Autonomous vehicles\n",
        "- Game-playing systems (e.g., AI agents)\n",
        "\n",
        "**2. Machine Learning (ML)**\n",
        "**Scope**\n",
        "- A subset of AI.\n",
        "- Enables machines to learn from data without being explicitly programmed.\n",
        "\n",
        "**Techniques**\n",
        "- Supervised, unsupervised, and reinforcement learning\n",
        "- Algorithms: Linear regression, decision trees, SVMs, random forests, clustering, etc.\n",
        "\n",
        "**Applications**\n",
        "- Email spam detection\n",
        "- Recommendation systems\n",
        "- Fraud detection\n",
        "- Predictive analytics\n",
        "- Customer churn prediction\n",
        "\n",
        "**3. Deep Learning (DL)**\n",
        "**Scope**\n",
        "- A subset of machine learning.\n",
        "- Uses multi-layered neural networks to learn complex patterns.\n",
        "\n",
        "**Techniques**\n",
        "- Neural networks: CNNs, RNNs, LSTMs, Transformers\n",
        "- Large-scale computation and big data\n",
        "\n",
        "**Applications**\n",
        "- Image and speech recognition\n",
        "- Natural language processing (NLP)\n",
        "- Self-driving cars\n",
        "- Medical image analysis\n",
        "- Generative AI (e.g., ChatGPT, DALL·E)\n",
        "\n",
        "**4. Data Science**\n",
        "\n",
        "**Scope**\n",
        "- A multidisciplinary field focused on extracting insights from data.\n",
        "- Includes AI/ML as tools but also involves statistics, data engineering, and domain knowledge.\n",
        "\n",
        "**Techniques**\n",
        "- Data cleaning and preprocessing\n",
        "- Statistical analysis\n",
        "- ML modeling\n",
        "- Data visualization\n",
        "\n",
        "**Applications**\n",
        "- Business analytics\n",
        "- Market analysis\n",
        "- Forecasting\n",
        "- Data-driven decision making\n",
        "\n"
      ],
      "metadata": {
        "id": "ShFefzrn4bmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Explain overfitting and underfitting in ML. How can you detect and prevent\n",
        "them?\n",
        "Hint: Discuss bias-variance tradeoff, cross-validation, and regularization techniques.\n",
        "**- 1. UNDERLIFTING**\n",
        "\n",
        " What it is\n",
        "- A model underfits when it is too simple to capture the underlying structure of the data.\n",
        "- It performs poorly on both the training set and the test set.\n",
        "\n",
        " Causes\n",
        "- Model is not complex enough (e.g., linear model for nonlinear data)\n",
        "- Too much regularization\n",
        "- Not enough training time or poor feature selection\n",
        "\n",
        "How to Detect\n",
        "- High bias: high error on training data\n",
        "- Training and validation errors are both high and close\n",
        "\n",
        "How to Prevent\n",
        "- Use a more complex model\n",
        "- Add more meaningful features\n",
        "\n",
        "**OVERFITTING**\n",
        "\n",
        "What it is\n",
        "- A model overfits when it learns the training data too well, including noise.\n",
        "- It performs extremely well on training data but poorly on validation/test data.\n",
        "\n",
        "Causes\n",
        "- Model is too complex\n",
        "- Not enough training data\n",
        "- Too many training epochs\n",
        "\n",
        "How to Detect\n",
        "- High variance: very low training error but high validation/test error\n",
        "- Learning curves diverge (training loss ↓ while validation loss ↑)\n",
        "\n",
        "How to Prevent\n",
        "- Regularization\n",
        "- Early stopping\n",
        "- Data augmentation (for images/text)\n",
        "\n",
        "**THE BIAS-VARIANCE TRADEOFF **\n",
        "\n",
        "Think of model training like learning a skill:\n",
        "- Someone who oversimplifies everything (high bias) → underfits\n",
        "- Someone who overthinks everything (high variance) → overfits\n",
        "- The best learner sits in the middle: smart enough to spot patterns, but not so obsessive that they memorize noise.\n",
        "- Cross-validation helps you find that middle ground."
      ],
      "metadata": {
        "id": "hi-yvmav9qmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. How would you handle missing values in a dataset? Explain at least three\n",
        "methods with examples.\n",
        "Hint: Consider deletion, mean/median imputation, and predictive modeling.\n",
        "\n",
        "- **1. Deletion Methods (Removing Data)**\n",
        "\n",
        "**A. Listwise Deletion (Remove Entire Rows)**\n",
        "- If a row contains missing values, delete the entire row.\n",
        "\n",
        "**When to use:**\n",
        "- Missing values are very few\n",
        "- Dataset is large\n",
        "- Data is missing completely at random (MCAR)\n",
        "\n",
        "=> example\n",
        "dataset\n",
        "\n",
        "| Age | Salary | City   |\n",
        "| --- | ------ | ------ |\n",
        "| 25  | 30,000 | Delhi  |\n",
        "| —   | 45,000 | Mumbai |\n",
        "| 30  | —      | Jaipur |\n",
        "after listwise deletion\n",
        "| Age | Salary | City  |\n",
        "| --- | ------ | ----- |\n",
        "| 25  | 30,000 | Delhi |\n",
        "\n",
        "**B. Column Deletion**\n",
        "- Delete a column if it contains too many missing values (70–90%).\n",
        "- Example:\n",
        "If the column “Middle Name” is 95% empty → drop it.\n",
        "\n",
        "**2. Mean/Median Imputation**\n",
        "- Replace missing values with statistical representatives\n",
        "\n",
        "** when to use **\n",
        "- For numerical data\n",
        "- When missing values are moderate\n",
        "- Mean: if data is normal\n",
        "- Median: if data is skewed or has outliers\n",
        "\n",
        "Example: Mean Imputation\n",
        "\"Age\" column:\n",
        "30, 25, —, 35\n",
        "Mean = (30 + 25 + 35) / 3 = 30\n",
        "So missing value becomes 30.\n",
        "\n",
        "Example: Median Imputation\n",
        "\n",
        "\"Income\" column:\n",
        "20k, 22k, —, 80k (outlier)\n",
        "Median = (20k + 22k) / 2 = 21k\n",
        "So missing value becomes 21k instead of mean (which would be inflated).\n",
        "\n",
        "** 3. Predictive Modeling (Advanced Imputation)**\n",
        "- Use machine learning to predict missing values.\n",
        "\n",
        "**When to use:**\n",
        "- Missing data is not random\n",
        "- Accuracy is important\n",
        "- Dataset is not too small\n",
        "\n",
        "**Methods**\n",
        "- Regression (for numerical values)\n",
        "- Classification (for categorical values)\n",
        "- K-Nearest Neighbors (KNN) Imputation\n",
        "- Random Forest Imputation\n",
        "\n",
        "Example: Regression Imputation\n",
        "- You want to fill missing values in Salary using Age and Experience:\n",
        "Select rows where Salary is not missing\n",
        "Train a regression model:\n",
        "Salary ~ Age + Experience\n",
        "Predict salary for rows with missing values\n",
        "\n",
        "Fill predicted values\n",
        "| Age | Experience | Salary |\n",
        "| --- | ---------- | ------ |\n",
        "| 25  | 1          | 30,000 |\n",
        "| 30  | 4          | —      |\n",
        "| 35  | 6          | 50,000 |\n",
        "\n",
        "Model predicts salary for Age 30, experience 4 → 40,000\n",
        "Imputed dataset:\n",
        "| Age | Experience | Salary |\n",
        "| --- | ---------- | ------ |\n",
        "| 25  | 1          | 30,000 |\n",
        "| 30  | 4          | 40,000 |\n",
        "| 35  | 6          | 50,000 |\n",
        "\n"
      ],
      "metadata": {
        "id": "KqmiCb81Cmcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is an imbalanced dataset? Describe two techniques to handle it\n",
        "(theoretical + practical).\n",
        "Hint: Discuss SMOTE, Random Under/Oversampling, and class weights in models.\n",
        "\n",
        "- A dataset is imbalanced when:\n",
        "- One class has far more samples than another\n",
        "- Example:\n",
        "| Class         | Count  |\n",
        "| ------------- | ------ |\n",
        "| Fraud (1)     | 200    |\n",
        "| Not Fraud (0) | 50,000 |\n",
        "\n",
        "**- TWO TECHNIQUES TO HANDLE IMBALANCE **\n",
        "\n",
        "**1. SMOTE (Synthetic Minority Oversampling Technique)**\n",
        "- SMOTE creates synthetic samples of the minority class, not by copying, but by generating new points between existing minority samples using K-NN neighbors.\n",
        "- This increases the minority class size in a more realistic and diverse way.\n",
        "\n",
        "**How SMOTE works:**\n",
        "- Pick a minority class point\n",
        "- Choose one of its K nearest minority neighbors\n",
        "- Create a synthetic point between the two\n",
        "- Repeat until classes are balanced\n",
        "\n",
        "**2. Random Under/Oversampling**\n",
        "\n",
        "**A. Random Oversampling**\n",
        "\n",
        "- Randomly duplicate samples from the minority class until the dataset becomes balanced.\n",
        "\n",
        "**Pros**\n",
        "- Simple\n",
        "- No information loss\n",
        "\n",
        "**Cons**\n",
        "- Risk of overfitting (same samples repeated)\n",
        "\n",
        "**B. Random Undersampling**\n",
        "- Randomly removes samples from the majority class to balance the dataset.\n",
        "\n",
        "**Pros**\n",
        "- Fast\n",
        "- Reduces dataset size (training becomes faster)\n",
        "\n",
        "**Cons**\n",
        "- Information loss (important samples removed)\n",
        "\n",
        "3. Class Weights (Cost-Sensitive Learning)\n",
        "\n",
        "- Instead of modifying data, modify the model.\n",
        "- Give more penalty to misclassifying minority class:\n",
        "- Misclassification of minority = high cost\n",
        "- Misclassification of majority = low cost"
      ],
      "metadata": {
        "id": "PB1mLbkAGLVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Why is feature scaling important in ML? Compare Min-Max scaling and\n",
        "Standardization.\n",
        "Hint: Explain impact on distance-based algorithms (e.g., KNN, SVM) and gradient\n",
        "descent.\n",
        "**- Importance of feature scaling in ML**\n",
        "\n",
        "**  1. Distance-based algorithms**\n",
        "- Algorithms like KNN, K-Means, SVM use Euclidean distance.\n",
        "- If one feature is on a larger scale, it dominates the distance calculation.\n",
        "- Example:\n",
        " Height: 150–180\n",
        " Income: 20,000–100,000\n",
        " Income will overpower height unless scaled.\n",
        "\n",
        "** 2. Gradient Descent Optimization**\n",
        "- Features with large scales → large gradients → unstable updates\n",
        "- Features with small scales → tiny gradients → slow learning\n",
        "- Scaling helps gradient descent converge faster and more smoothly.\n",
        "\n",
        "** 3. Regularization**\n",
        "- Models like Lasso & Ridge penalize large coefficients.\n",
        "- Without scaling, features with large scales get unfair penalties.\n",
        "\n",
        "**4. Improved model performance & stability**\n",
        "- Scaling reduces numerical instability and often improves accuracy.\n",
        "\n",
        "**Min–Max Scaling vs Standardization**\n",
        "\n",
        "** 1. Min–Max Scaling (Normalization)**\n",
        "- Transforms features to a fixed range (usually 0 to 1)\n",
        "- Xscaled = X - Xmin % Xmax - Xmin\n",
        "\n",
        "**Characteristics**\n",
        "- Keeps shape of distribution\n",
        "- Sensitive to outliers\n",
        "- All values lie in [0,1]\n",
        "\n",
        "** When to Use**\n",
        "- Neural networks\n",
        "- Algorithms requiring bounded inputs\n",
        "- When the data has no major outliers\n",
        "\n",
        "** Example**\n",
        "- Income: 20,000 → 1,00,000\n",
        "- 20,000 will become 0, and 1,00,000 will become 1.\n",
        "\n",
        "**2. Standardization (Z-score Scaling)**\n",
        "\n",
        "Transforms features to mean = 0 and standard deviation = 1:\n",
        " - Z= X−μ​ % σ\n",
        "\n",
        "**Characteristics**\n",
        "- Centers data around 0\n",
        "- Not bounded (values can be negative or >1)\n",
        "- Less sensitive to outliers\n",
        "\n",
        "** When to Use**\n",
        "- SVM\n",
        "- Logistic Regression\n",
        "- Linear Regression\n",
        "\n",
        "\n",
        "\t​\n"
      ],
      "metadata": {
        "id": "2WnOOXT5JYCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Compare Label Encoding and One-Hot Encoding. When would you prefer\n",
        "one over the other?\n",
        "Hint: Consider categorical variables with ordinal vs. nominal relationships.\n",
        "\n",
        "- Assigns each category an integer value.\n",
        "\n",
        "- Example:\n",
        " For feature Size = {Small, Medium, Large}\n",
        " Label Encoding →\n",
        " Small = 0\n",
        " Medium = 1\n",
        " Large = 2\n",
        "\n",
        "** Pros**\n",
        "- Simple to implement\n",
        "- No increase in number of columns\n",
        "- Works well with tree-based models (Decision Tree, Random Forest, XGBoost)\n",
        "\n",
        "**Cons**\n",
        "- Imposes a false ordinal relationship\n",
        "- Continuous models (Linear Regression, KNN, SVM) may misunderstand encoded integers as having mathematical meaning\n",
        "\n",
        "**When to Use Label Encoding**\n",
        "- When categories have natural order (ordinal)\n",
        "  Examples:\n",
        "- Size (Small < Medium < Large)\n",
        "- Education Level (High School < Bachelor < Master < PhD)\n",
        "- Rating (Bad < Average < Good)\n",
        "- When using tree-based algorithms, which do not rely on distance\n",
        "\n",
        "**2. One-Hot Encoding**\n",
        "- Creates binary columns (0/1) for each category.\n",
        "- Example:\n",
        "  For feature Color = {Red, Blue, Green}\n",
        "  One-Hot Encoding →\n",
        "\n",
        "  | Red | Blue | Green |\n",
        "| --- | ---- | ----- |\n",
        "| 1   | 0    | 0     |\n",
        "| 0   | 1    | 0     |\n",
        "| 0   | 0    | 1     |\n",
        "\n",
        "**Pros**\n",
        "- No false order introduced\n",
        "- Works well with distance-based and linear algorithms\n",
        "- Treats all categories equally\n",
        "\n",
        "**Cons**\n",
        "- Increases dimensionality\n",
        "- Can cause sparsity when categories are many (e.g., thousands of unique values)"
      ],
      "metadata": {
        "id": "uRSyX-fHQJ4v"
      }
    }
  ]
}